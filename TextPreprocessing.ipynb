{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "Preproccesing is very important for NLP.The better we do text preprocessing, the better our model will yield.We can perform different operations depending on the model we will use.\n"
      ],
      "metadata": {
        "id": "7Rb6l2GvWqkb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "leyUJf2fKCn1"
      },
      "outputs": [],
      "source": [
        "import nltk \n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_str= \"Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\""
      ],
      "metadata": {
        "id": "yN7CXn2GMNdn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lower Casing"
      ],
      "metadata": {
        "id": "DHV2134lWGq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_text(text):\n",
        "  return text.lower()\n",
        "  "
      ],
      "metadata": {
        "id": "52fFot9nMb17"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lowercase_text(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "tAppAGnRMkUp",
        "outputId": "6109b53e-2a39-4540-df3e-a7234e6e52b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'recurrent models typically factor computation along the symbol positions of the input and output sequences. aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. this inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. the fundamental constraint of sequential computation, however, remains.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removal Numerical Value"
      ],
      "metadata": {
        "id": "pmJ1uPr2WOuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_num(text):\n",
        "  result= re.sub(r'\\d+', '' , text)\n",
        "  return result"
      ],
      "metadata": {
        "id": "kqso_7KnMqUm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_num(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "Q2CbvhJkNLIt",
        "outputId": "9f75427a-21b3-44d9-939b-b7ea28357d93"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht− and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [] and conditional computation [], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removal of Punctuations"
      ],
      "metadata": {
        "id": "jkvs4SyuWU4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punct(text):\n",
        "  translator=str.maketrans('','', string.punctuation)\n",
        "  return text.translate(translator)"
      ],
      "metadata": {
        "id": "q7p9dhO4NQZ5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_punct(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "YTCP3FmlN6Wu",
        "outputId": "680fe124-76cc-48fd-e902-3e4029fb6670"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Recurrent models typically factor computation along the symbol positions of the input and output sequences Aligning the positions to steps in computation time they generate a sequence of hidden states ht as a function of the previous hidden state ht−1 and the input for position t This inherently sequential nature precludes parallelization within training examples which becomes critical at longer sequence lengths as memory constraints limit batching across examples Recent work has achieved significant improvements in computational efficiency through factorization tricks 21 and conditional computation 32 while also improving model performance in case of the latter The fundamental constraint of sequential computation however remains'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpIsq-XXQCZK",
        "outputId": "20f46f50-5dd8-49d8-b48c-abacf7aa5313"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a00kCJ92QHPk",
        "outputId": "390f9b96-d765-4621-f763-0c57b807222c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xTByclNN9OK",
        "outputId": "b5be1a70-46a9-46a4-b518-b4b9f4443dc6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removal Stopwords and Tokenize"
      ],
      "metadata": {
        "id": "b3IUhZmRWZC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rem_stopwords(text):\n",
        "  stop_words =set(stopwords.words('english'))\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filltered_text= [word for word in word_tokens if word not in stop_words]\n",
        "  return filltered_text"
      ],
      "metadata": {
        "id": "fYfPMW9vO0gn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=rem_stopwords(input_str)"
      ],
      "metadata": {
        "id": "CAGT5RkaPtrV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dmUG7PLRlp_",
        "outputId": "af9e9318-3da6-49c5-bf95-b901c995b749"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Recurrent',\n",
              " 'models',\n",
              " 'typically',\n",
              " 'factor',\n",
              " 'computation',\n",
              " 'along',\n",
              " 'symbol',\n",
              " 'positions',\n",
              " 'input',\n",
              " 'output',\n",
              " 'sequences',\n",
              " '.',\n",
              " 'Aligning',\n",
              " 'positions',\n",
              " 'steps',\n",
              " 'computation',\n",
              " 'time',\n",
              " ',',\n",
              " 'generate',\n",
              " 'sequence',\n",
              " 'hidden',\n",
              " 'states',\n",
              " 'ht',\n",
              " ',',\n",
              " 'function',\n",
              " 'previous',\n",
              " 'hidden',\n",
              " 'state',\n",
              " 'ht−1',\n",
              " 'input',\n",
              " 'position',\n",
              " 't.',\n",
              " 'This',\n",
              " 'inherently',\n",
              " 'sequential',\n",
              " 'nature',\n",
              " 'precludes',\n",
              " 'parallelization',\n",
              " 'within',\n",
              " 'training',\n",
              " 'examples',\n",
              " ',',\n",
              " 'becomes',\n",
              " 'critical',\n",
              " 'longer',\n",
              " 'sequence',\n",
              " 'lengths',\n",
              " ',',\n",
              " 'memory',\n",
              " 'constraints',\n",
              " 'limit',\n",
              " 'batching',\n",
              " 'across',\n",
              " 'examples',\n",
              " '.',\n",
              " 'Recent',\n",
              " 'work',\n",
              " 'achieved',\n",
              " 'significant',\n",
              " 'improvements',\n",
              " 'computational',\n",
              " 'efficiency',\n",
              " 'factorization',\n",
              " 'tricks',\n",
              " '[',\n",
              " '21',\n",
              " ']',\n",
              " 'conditional',\n",
              " 'computation',\n",
              " '[',\n",
              " '32',\n",
              " ']',\n",
              " ',',\n",
              " 'also',\n",
              " 'improving',\n",
              " 'model',\n",
              " 'performance',\n",
              " 'case',\n",
              " 'latter',\n",
              " '.',\n",
              " 'The',\n",
              " 'fundamental',\n",
              " 'constraint',\n",
              " 'sequential',\n",
              " 'computation',\n",
              " ',',\n",
              " 'however',\n",
              " ',',\n",
              " 'remains',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "cnt = Counter()\n",
        "for text in a:\n",
        "    for word in text.split():\n",
        "        cnt[word] += 1\n",
        "        \n",
        "cnt.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkzLNu8OQy_Y",
        "outputId": "7b8f23b0-5495-4aa2-842c-df826b2433bd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 7),\n",
              " ('computation', 4),\n",
              " ('.', 4),\n",
              " ('positions', 2),\n",
              " ('input', 2),\n",
              " ('sequence', 2),\n",
              " ('hidden', 2),\n",
              " ('sequential', 2),\n",
              " ('examples', 2),\n",
              " ('[', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S"
      ],
      "metadata": {
        "id": "4mASsadkTT-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming "
      ],
      "metadata": {
        "id": "OPRgKvduTVe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "J1qk9wbzRd5o"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]"
      ],
      "metadata": {
        "id": "NNFUq8NgSQi0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in example_words:\n",
        "    print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-aXLwkYSI__",
        "outputId": "fbc0a51e-1b5e-41a8-a798-e6b8528ffa89"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python\n",
            "python\n",
            "python\n",
            "python\n",
            "pythonli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\""
      ],
      "metadata": {
        "id": "K7TeKI1PSKAq"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(new_text)\n",
        "\n",
        "for w in words:\n",
        "    print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t82TXmmmTP_a",
        "outputId": "57e1deb7-c3a5-4711-840d-3a8ae9398b72"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it\n",
            "is\n",
            "import\n",
            "to\n",
            "by\n",
            "veri\n",
            "pythonli\n",
            "while\n",
            "you\n",
            "are\n",
            "python\n",
            "with\n",
            "python\n",
            ".\n",
            "all\n",
            "python\n",
            "have\n",
            "python\n",
            "poorli\n",
            "at\n",
            "least\n",
            "onc\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "UDQVDZYSTsQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "  \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "  \n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "print(\"worst :\", lemmatizer.lemmatize(\"worst\",pos=\"a\"))  \n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWExLJ-sTR_o",
        "outputId": "a4995bc2-993c-4e89-c320-96f6d6d54570"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "worst : bad\n",
            "better : good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YNLQIgAcUn8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversion of Emoji to Words"
      ],
      "metadata": {
        "id": "TmR2fIqoU4rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removal of Emojis"
      ],
      "metadata": {
        "id": "zIzUZf63UtMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "remove_emoji(\"game is on 🔥🔥\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cRLjOdZ2TyxW",
        "outputId": "e9c4180c-a1d4-41c6-b6a1-895eec248cff"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'game is on '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removal of URLS "
      ],
      "metadata": {
        "id": "nglSLYlsVcUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)"
      ],
      "metadata": {
        "id": "YbLWr9X0VZ6u"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Driverless AI NLP blog post on https://www.h2o.ai/blog/detecting-sarcasm-is-difficult-but-ai-may-have-an-answer/\"\n",
        "remove_urls(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "R-skWxKlVCAG",
        "outputId": "9bb7626e-c334-4fad-a16e-2db5057b1558"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Driverless AI NLP blog post on '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Want to know more. Checkout www.h2o.ai for additional information\"\n",
        "remove_urls(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "FSHDWpxBVj3_",
        "outputId": "1be196ec-473d-4b76-b78e-19a2a381d63d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Want to know more. Checkout  for additional information'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removal of HTML Tags"
      ],
      "metadata": {
        "id": "p-aVGE1NVrQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html(text):\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    return html_pattern.sub(r'', text)\n",
        "\n",
        "text = \"\"\"<div>\n",
        "<h1> H2O</h1>\n",
        "<p> AutoML</p>\n",
        "<a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\"> Driverless AI</a>\n",
        "</div>\"\"\"\n",
        "\n",
        "print(remove_html(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GeXPZXDVsQo",
        "outputId": "41b85d30-4ebd-491a-8da9-b1247cfc3134"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " H2O\n",
            " AutoML\n",
            " Driverless AI\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Words Conversion"
      ],
      "metadata": {
        "id": "GNeUFvGrV3hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_words_str = \"\"\"\n",
        "AFAIK=As Far As I Know\n",
        "AFK=Away From Keyboard\n",
        "ASAP=As Soon As Possible\n",
        "ATK=At The Keyboard\n",
        "ATM=At The Moment\n",
        "A3=Anytime, Anywhere, Anyplace\n",
        "BAK=Back At Keyboard\n",
        "BBL=Be Back Later\n",
        "BBS=Be Back Soon\n",
        "BFN=Bye For Now\n",
        "B4N=Bye For Now\n",
        "BRB=Be Right Back\n",
        "BRT=Be Right There\n",
        "BTW=By The Way\n",
        "B4=Before\n",
        "B4N=Bye For Now\n",
        "CU=See You\n",
        "CUL8R=See You Later\n",
        "CYA=See You\n",
        "FAQ=Frequently Asked Questions\n",
        "FC=Fingers Crossed\n",
        "FWIW=For What It's Worth\n",
        "FYI=For Your Information\n",
        "GAL=Get A Life\n",
        "GG=Good Game\n",
        "GN=Good Night\n",
        "GMTA=Great Minds Think Alike\n",
        "GR8=Great!\n",
        "G9=Genius\n",
        "IC=I See\n",
        "ICQ=I Seek you (also a chat program)\n",
        "ILU=ILU: I Love You\n",
        "IMHO=In My Honest/Humble Opinion\n",
        "IMO=In My Opinion\n",
        "IOW=In Other Words\n",
        "IRL=In Real Life\n",
        "KISS=Keep It Simple, Stupid\n",
        "LDR=Long Distance Relationship\n",
        "LMAO=Laugh My A.. Off\n",
        "LOL=Laughing Out Loud\n",
        "LTNS=Long Time No See\n",
        "L8R=Later\n",
        "MTE=My Thoughts Exactly\n",
        "M8=Mate\n",
        "NRN=No Reply Necessary\n",
        "OIC=Oh I See\n",
        "PITA=Pain In The A..\n",
        "PRT=Party\n",
        "PRW=Parents Are Watching\n",
        "ROFL=Rolling On The Floor Laughing\n",
        "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
        "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
        "SK8=Skate\n",
        "STATS=Your sex and age\n",
        "ASL=Age, Sex, Location\n",
        "THX=Thank You\n",
        "TTFN=Ta-Ta For Now!\n",
        "TTYL=Talk To You Later\n",
        "U=You\n",
        "U2=You Too\n",
        "U4E=Yours For Ever\n",
        "WB=Welcome Back\n",
        "WTF=What The F...\n",
        "WTG=Way To Go!\n",
        "WUF=Where Are You From?\n",
        "W8=Wait...\n",
        "7K=Sick:-D Laugher\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8e78ynCfVvKY"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_words_map_dict = {}\n",
        "chat_words_list = []\n",
        "for line in chat_words_str.split(\"\\n\"):\n",
        "    if line != \"\":\n",
        "        cw = line.split(\"=\")[0]\n",
        "        cw_expanded = line.split(\"=\")[1]\n",
        "        chat_words_list.append(cw)\n",
        "        chat_words_map_dict[cw] = cw_expanded\n",
        "chat_words_list = set(chat_words_list)\n",
        "\n",
        "def chat_words_conversion(text):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words_list:\n",
        "            new_text.append(chat_words_map_dict[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "chat_words_conversion(\"one minute BRB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "aKZdo4pbV8gA",
        "outputId": "a005a14f-2485-4ee2-9bd2-9809c9364eb7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'one minute Be Right Back'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7qsap78IV-na"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}